{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final code for capturing discriminative features to be written here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all file in this block\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from pprint import pprint\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaration of global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#readDataFiles : reads the file and return list of list of words, and label if necessary\n",
    "#path_var: variable for path of file to be read, mode: type of data(test - 1 or trian - 0)\n",
    "def readDataFiles(path_var, mode):\n",
    "    data = []\n",
    "    label = []\n",
    "    with open(path_var , 'r') as file_obj:\n",
    "        for line in file_obj:\n",
    "            temp = line.split(',')\n",
    "            data.append(temp[:3])\n",
    "            if(1 != mode):\n",
    "                label.append(temp[-1][0])\n",
    "    if(1 != mode):\n",
    "        return data, label\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generateModel : generates the word2vec model for the data gathered\n",
    "def generateModel(train_set, val_set, test_set):\n",
    "    word_set = train_set + val_set + test_set\n",
    "    word_model = gensim.models.Word2Vec(word_set, min_count = 1, size = 30, window = 5)\n",
    "    return word_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "#word_model = None\n",
    "def word_embedding(word_model):\n",
    "    print('call')\n",
    "#     global word_model\n",
    "    train_path = \"./training/train.txt\"\n",
    "    train_set, label_train = readDataFiles(train_path, 0)\n",
    "    #print(train_set)\n",
    "\n",
    "    for li in train_set:\n",
    "        word1 = li[0]\n",
    "        word2 = li[1]\n",
    "        word3 = li[2]\n",
    "        \n",
    "    embd1 = word_model[word1]\n",
    "    embd2 = word_model[word2]\n",
    "    embd3 = word_model[word3]\n",
    "    \n",
    "    embd_list1 = embd1.tolist()\n",
    "    embd_list2 = embd2.tolist()\n",
    "    embd_list3 = embd3.tolist()\n",
    "    \n",
    "    embd_comb_list = embd_list1 + embd_list2 + embd_list3\n",
    "    embd_comb_arr = np.array(embd_comb_list)\n",
    "    print(len(embd_comb_list))\n",
    "    print(embd_comb_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_train(x, w):\n",
    "    z = np.dot(x,w)\n",
    "    g = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_gradient_train(x, y):\n",
    "    itr = 0\n",
    "    lr = 0.01 \n",
    "    #w = np.zeros(x.shape[0])\n",
    "    w = np.empty(x.shape[0])\n",
    "    w.fill(1)\n",
    "    #print(w)\n",
    "    h = predicted_train(x,w)\n",
    "    gradient = np.dot(x.T, (y - h))\n",
    "    w = w - lr * gradient\n",
    "        \n",
    "            \n",
    "    return w\n",
    "\n",
    "\n",
    "#gradient = np.dot(model['Fulton'], (1 - 0.5)) \n",
    "# print(gradient)\n",
    "wout =  execute_gradient_train(model['Fulton'] , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mat(prob_arr, input_arr):\n",
    "        # confusion matrix\n",
    "        conf_arr = [[0, 0], [0, 0]]\n",
    "\n",
    "        for i in range(len(prob_arr)):\n",
    "                if int(input_arr[i]) == 0:\n",
    "                        if float(prob_arr[i]) < 0.5:\n",
    "                                conf_arr[0][1] = conf_arr[0][1] + 1\n",
    "                        else:\n",
    "                                conf_arr[0][0] = conf_arr[0][0] + 1\n",
    "                elif int(input_arr[i]) == 1:\n",
    "                        if float(prob_arr[i]) >= 0.5:\n",
    "                                conf_arr[1][0] = conf_arr[1][0] +1\n",
    "                        else:\n",
    "                                conf_arr[1][1] = conf_arr[1][1] +1\n",
    "\n",
    "        return conf_arr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(conf_mat,leng):\n",
    "    accu=conf_mat[0][0]+conf_mat[1][1]/leng\n",
    "    return accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #global word_model\n",
    "    #setting of path to take data\n",
    "    train_path = \"./training/train.txt\"\n",
    "    val_path = \"./training/validation.txt\"\n",
    "    test_path = \"./test/test_triples.txt\"\n",
    "    \n",
    "    #reading data from the respective files\n",
    "    train_set, label_train = readDataFiles(train_path, 0)\n",
    "    val_set, label_val = readDataFiles(val_path, 0)\n",
    "    test_set = readDataFiles(test_path, 1)\n",
    "    \n",
    "    #generating word2vec model for the data\n",
    "    word_model = generateModel(train_set, val_set, test_set)\n",
    "    #print(word_model['duck'])\n",
    "    #print(word_model['goose'])\n",
    "    print(word_model['sticky'])\n",
    "\n",
    "\n",
    "    # assert word_model is not None\n",
    "    word_embedding(word_model)\n",
    "    #Once we get the prob_arr, we pass both input_arr which is the class labels and the prob_arr as param\n",
    "    conf_arr[][]=conf_mat(prob_arr, input_arr)\n",
    "    leng=len(input_arr)\n",
    "    acc=calc_accuracy(conf_mat,leng)\n",
    "    #Similarly we'll calculate for all the other parameters if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00841915  0.04042975 -0.01581143 -0.04377678 -0.01125767  0.00974675\n",
      "  0.00147352  0.00744274  0.09583332  0.00634465 -0.01631458  0.05775813\n",
      " -0.00619441 -0.02274013 -0.00458141 -0.00753357 -0.0481015   0.04299363\n",
      " -0.00080485 -0.00478973 -0.01256828  0.01597079 -0.02835188 -0.01430109\n",
      " -0.00047631 -0.00512271 -0.00626    -0.04607755 -0.01368228  0.03558755]\n",
      "call\n",
      "90\n",
      "[ 0.08560743  0.63886511 -0.36200163 -0.50699699 -0.09832829  0.08850225\n",
      " -0.14605245 -0.04926843  1.05019963  0.21008919 -0.48424104  0.49639779\n",
      "  0.00480673 -0.1088439   0.12956478 -0.27704149 -0.63833356  0.40594438\n",
      "  0.10624984 -0.04095186  0.01408579  0.21265791 -0.44325998 -0.01008157\n",
      " -0.15063983  0.02850275 -0.02531501 -0.56696278 -0.25552472  0.57662433\n",
      "  0.10066018  0.80332613 -0.48652893 -0.60507464 -0.11407175  0.09941965\n",
      " -0.15195854  0.04118977  1.27047074  0.31803095 -0.56808746  0.71282864\n",
      "  0.06282917 -0.09528975  0.2322416  -0.26960424 -0.81898111  0.62286848\n",
      "  0.0208778  -0.02460421  0.07633203  0.27016714 -0.54015058  0.10346356\n",
      " -0.29422197  0.072603   -0.01106914 -0.69556224 -0.33518893  0.6318543\n",
      "  0.01245192  0.33245838 -0.19521746 -0.2153661  -0.10913924  0.02286477\n",
      " -0.06354736 -0.01680909  0.5228411   0.12701133 -0.21504351  0.27060825\n",
      "  0.02193922 -0.04677209  0.06895017 -0.11273859 -0.32332084  0.2049876\n",
      "  0.04217533 -0.02550737  0.00496188  0.12479526 -0.21889828  0.01778205\n",
      " -0.08155496  0.00964995  0.01009397 -0.28371519 -0.13120645  0.28452891]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/base_py_environments/first_env/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/user/base_py_environments/first_env/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/user/base_py_environments/first_env/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n",
      "/home/user/base_py_environments/first_env/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_train(x, w):\n",
    "    z = np.dot(x,w)\n",
    "    g = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_gradient_train(x, y):\n",
    "    itr = 0\n",
    "    lr = 0.01 \n",
    "    #w = np.zeros(x.shape[0])\n",
    "    w = np.empty(x.shape[0])\n",
    "    w.fill(1)\n",
    "    #print(w)\n",
    "    h = predicted_train(x,w)\n",
    "    gradient = np.dot(x.T, (y - h))\n",
    "    w = w - lr * gradient\n",
    "        \n",
    "            \n",
    "    return w\n",
    "\n",
    "\n",
    "#gradient = np.dot(model['Fulton'], (1 - 0.5)) \n",
    "# print(gradient)\n",
    "#goose,duck,sticky\n",
    "\n",
    "#wout =  execute_gradient_train(model['Fulton'] , 1)\n",
    "#print(wout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-59cd2179617c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membd1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#word_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mword_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'word_model' is not defined"
     ]
    }
   ],
   "source": [
    "def word_embedding(word_model):\n",
    "    \n",
    "#     global word_model\n",
    "    train_path = \"./training/train.txt\"\n",
    "    train_set, label_train = readDataFiles(train_path, 0)\n",
    "    #print(train_set)\n",
    "\n",
    "    for li in train_set:\n",
    "        word1 = li[0]\n",
    "        word2 = li[1]\n",
    "        word3 = li[2]\n",
    "        \n",
    "    embd1 = word_model[word1]\n",
    "    embd2 = word_model[word2]\n",
    "    embd2 = word_model[word3]\n",
    "    print(embd1)\n",
    "#word_model\n",
    "# wm = wo\n",
    "word_embedding(word_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
